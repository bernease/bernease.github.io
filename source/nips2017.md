---
layout: page
title: NIPS 2017 Information
permalink: /nips2017-info
---

< [*Back to Bernease Herman's (mostly empty) homepage*](http://www.berneaseherman.com)

## Where I'm (likely) hanging out at NIPS 2017

**Thursday, Dec 7:**
`2:00-9:30pm` | Symposium on Interpretable Machine Learning, *Hall C*, *Long Beach Convention Center* [[webpage](http://interpretable.ml)]

**Friday, Dec 8:**
`8:50am-12:30pm` | Morning session of Transparent and Interpretable ML in Safety Critical Environments workshop, *Room 201A*, *Long Beach Convention Center* [[webpage](http://sites.google.com/view/timl-nips2017)]
`1:00-8:30pm` | Black in AI workshop and dinner, *Pike Ballroom*, *Marriott Renaissance Long Beach Hotel* [[webpage](http://blackinai.org)]

**Saturday, Dec 9:**
`8:15-9:15am` | Been Kim's talk at Interpreting, Explaining and Visualizing Deep Learning, *Regency Ballroom A+B+C*, *Hyatt Hotel*, [[website](http://www.interpretable-ml.org/nips2017workshop)]
`9:45am-2:15pm` | Cognitively Informed Artificial Intelligence (CIAI) workshop, *Room 104A*, *Long Beach Convention Center*, [[website](https://sites.google.com/view/ciai2017/)]
`2:15-3:00pm` | Poster session for Aligned Artificial Intelligence, *Room 201B*, *Long Beach Convention Center*
`3:00-6:30pm` | Back to CIAI workshop
`7:30-9:30pm` | Tentatively organizing a dinner for select Black in AI, Interpretable ML, and AI Ethics people, *Location unknown*, *email (*`<firstname>@gmail.com`) *or tweet ([@bernease](http://www.twitter.com/bernease)) for details*

### The Promise and Perils of Human Evaluation for Model Interpretability

#### Abstract

Transparency, user trust, and human comprehension are popular ethical motivations for interpretable machine learning. In support of these goals, researchers evaluate model explanation performance using humans and real world applications.  However, these functional measures of interpretability are likely to introduce bias with regard to specific users, applications, and explanation vehicles. Evaluation and optimization using human measures perpetuate implicit cognitive bias in explanations that threaten transparency. We propose two potential research directions to disambiguate cognitive function and explanation models, regaining control over the tradeoff between accuracy and interpretability.

#### Paper

The Promise and Peril of Human Evaluation for Model Interpretability
[[arXiv](https://arxiv.org/abs/1711.07414)] [[pdf](https://arxiv.org/pdf/1711.07414.pdf)]

Published on arXiv as a part of the Proceedings of NIPS 2017 Symposium on Interpretable Machine Learning
[[full proceedings](https://arxiv.org/abs/1711.09889)]

#### Poster

Extended poster [[png](http://berneaseherman.com/assets/images/extendedNIPS17poster.png)]. Presented at NIPS 2017 Symposium on Interpretable Machine Learning.

![Extended poster](http://berneaseherman.com/assets/images/extendedNIPS17poster.png)

Original Poster [[png](http://berneaseherman.com/assets/images/originalNIPS17poster.png)]. Presented at the Black In AI workshop co-located with NIPS 2017.

![Original poster](http://berneaseherman.com/assets/images/originalNIPS17poster.png)
